# Developer Guide â€“ Knowledge Flow Backend

This guide explains how to understand, extend, and contribute to the Knowledge Flow microservice.

---

## API Structure: REST and MCP

The `knowledge_flow_app` exposes two API namespaces for different purposes:

---

### 1. REST API

- **Base URL**: `/knowledge/v1`
- **Used by**: React frontend, CLI tools, admin scripts
- **Includes**:
  - Ingestion endpoints
  - Metadata management
  - Vector search
  - Tabular schema/query
  - Raw content access

You can access the Swagger UI at:

```
http://localhost:8111/knowledge/v1/docs
```

---

### 2. MCP API

- **Base URL**: `/mcp` (not nested under `/knowledge/v1`)
- **Used by**: Agents (e.g., Dominic) that follow the [LangGraph MCP spec](https://github.com/langchain-ai/langgraph/tree/main/libs/langgraph/experimental/mcp)
- **Exposes only tagged endpoints**:
  - `Vector Search`
  - `Tabular`

Mounted via `FastApiMCP` in `main.py`:

```python
mcp = FastApiMCP(
    app,
    name="Knowledge Flow MCP",
    description="MCP server for Knowledge Flow",
    include_tags=["Vector Search", "Tabular"],
    describe_all_responses=True,
    describe_full_response_schema=True,
)
mcp.mount()  # Mounts the /mcp namespace
```

### Design Rationale

We deliberately **separate REST and MCP namespaces**:

- âœ… Easier to distinguish agent-facing endpoints
- âœ… Enables differential access control, observability, or documentation
- âœ… Keeps the REST API clean and focused on user/UI/system integrations

As a result, all MCP endpoints follow this pattern:

```
/mcp/vector/search
/mcp/tabular/{document_uid}/schema
/mcp/tabular/{document_uid}/query
/mcp/tabular/list
```

Even though the `VectorSearchController` and `TabularController` do **not** hardcode `/mcp/` in their route paths, MCP tagging ensures proper exposure.

This approach is consistent and scalable across all agent-facing interfaces.


## Processor Architecture

### Input Processors

Input processors extract structured content and metadata from uploaded files.

| Type     | Location                     | Output        |
|----------|------------------------------|---------------|
| Markdown | `input_processors/`          | Markdown text |
| Tabular  | `input_processors/`          | Table rows    |

Each input processor specializes in a file type (PDF, DOCX, CSV, etc.).

### Output Processors

Output processors transform parsed content into embeddings or structured records.

| Type         | Location                       | Output       |
|--------------|--------------------------------|--------------|
| Vectorization| `output_processors/vectorization_processor/` | Embeddings + metadata |
| Tabular      | `output_processors/tabular_processor/`       | Normalized records    |

---

## Vectorization Pipeline

```txt
  [ FILE PATH + METADATA ]
          â”‚
          â–¼
  DocumentLoaderInterface
          â”‚
          â–¼
   TextSplitterInterface
          â”‚
          â–¼
  EmbeddingModelInterface
          â”‚
          â–¼
   VectorStoreInterface
          â”‚
          â–¼
   BaseMetadataStore
```

Each interface is pluggable. You can switch OpenSearch â†’ Pinecone, or Azure â†’ HuggingFace by updating config and implementing the interface.

---

## Knowledge Flow â€“ Project Layout (Modularized)

This project uses a modular architecture that separates domain features from core infrastructure. Below is the directory structure with roles and guidelines for extension.

---

### ğŸ”© Root Layout

```
knowledge_flow_app/
â”œâ”€â”€ main.py                       # FastAPI + MCP entrypoint
â”œâ”€â”€ application_context.py        # Shared runtime context (DI / singleton-style config)
â”œâ”€â”€ config/                       # YAML and Python-based configuration modules
â”œâ”€â”€ common/                       # Shared types, utilities, exceptions
â”œâ”€â”€ features/                     # DOMAIN LOGIC â€” Controllers, Services, Structures
â”œâ”€â”€ core/                         # INFRASTRUCTURE â€” Stores, Processors, Pipelines
â””â”€â”€ services/ (optional)          # Cross-cutting service logic if needed
```

---

### `features/` â€“ Domain-Centric Components

Each folder inside `features/` implements a full vertical slice:

```
features/
â”œâ”€â”€ tabular/
â”‚   â”œâ”€â”€ controller.py             # TabularController (CSV schema, query, list)
â”‚   â”œâ”€â”€ service.py                # TabularService
â”‚   â””â”€â”€ structures.py            # Pydantic models for requests/responses
â”œâ”€â”€ vector_search/
â”‚   â”œâ”€â”€ controller.py
â”‚   â”œâ”€â”€ service.py
â”‚   â””â”€â”€ structures.py
â””â”€â”€ metadata/
    â”œâ”€â”€ controller.py
    â”œâ”€â”€ service.py
    â””â”€â”€ structures.py
```

ğŸ“Œ **Where to add a new API?**
- Create a folder in `features/`
- Add controller, service, and structures as needed

---

### `core/` â€“ Infrastructure + Processing

```
core/
â”œâ”€â”€ stores/
â”‚   â”œâ”€â”€ base_metadata_store.py
â”‚   â”œâ”€â”€ base_content_store.py
â”‚   â”œâ”€â”€ vector_store/
â”‚   â”‚   â”œâ”€â”€ opensearch_vector_store.py
â”‚   â”‚   â””â”€â”€ in_memory_vector_store.py
â”‚   â””â”€â”€ metadata_store/
â”‚       â”œâ”€â”€ local_metadata_store.py
â”‚       â””â”€â”€ opensearch_metadata_store.py
â”‚
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ input/
â”‚   â”‚   â”œâ”€â”€ csv/
â”‚   â”‚   â”‚   â””â”€â”€ csv_tabular_processor.py
â”‚   â”‚   â”œâ”€â”€ docx/
â”‚   â”‚   â”œâ”€â”€ pdf/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ output/
â”‚       â”œâ”€â”€ vectorization/
â”‚       â”‚   â””â”€â”€ vectorization_processor.py
â”‚       â”œâ”€â”€ tabular_normalization/
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ pipelines/
    â”œâ”€â”€ vectorization_pipeline.py
    â”œâ”€â”€ tabular_pipeline.py
    â””â”€â”€ ...
```

ğŸ“Œ **Where to add new file support?**
- Add an `input/your_type/` folder inside `core/processors`

ğŸ“Œ **Where to add embedding or normalization logic?**
- Use `core/pipelines/` and optionally create helpers in `output/`

---

### `common/` â€“ Shared Types and Helpers

```
common/
â”œâ”€â”€ business_exception.py         # Domain-specific exceptions
â”œâ”€â”€ structures.py                 # Shared Pydantic models
â””â”€â”€ utils.py                      # Generic helpers
```

---

### `config/` â€“ Pluggable Settings

Contains config classes (e.g., OpenAI, Ollama, GCS, MinIO):

```
config/
â”œâ”€â”€ embedding_openai_settings.py
â”œâ”€â”€ content_store_minio_settings.py
â”œâ”€â”€ opensearch_settings.py
â””â”€â”€ ...
```

---

### Developer Guidance

| Task                          | Location                                      |
|-------------------------------|-----------------------------------------------|
| Add a new REST API            | `features/your_feature/`                      |
| Add support for a new file    | `core/processors/input/your_type/`            |
| Add a new vector store        | `core/stores/vector_store/`                   |
| Change embedding backend      | `core/pipelines/vectorization_pipeline.py`    |
| Customize ingestion flow      | `services/ingestion_service.py`               |
| Add business exceptions       | `common/business_exception.py`               |

---

## âœ… Dev loop

```bash
make dev         # Setup .venv
make run         # Start FastAPI server
make test        # Run pytest
```

---

## ğŸ§ª Testing

Use standard `pytest`. Tests live in `tests/`.

Run all:
```bash
pytest
```

Run specific:
```bash
pytest tests/test_vector_search_service.py
```

---

## ğŸ“š Docs and Swagger

Docs available at: [http://localhost:8111/knowledge/v1/docs](http://localhost:8111/knowledge/v1/docs)

---

## ğŸ¤ Contributions

Please follow the coding guidelines in `CODING_GUIDELINES.md` and submit PRs or issues via GitHub.

## Testing

```bash
make test
```

Use standard `pytest`. Tests live in `tests/`, and you can run individual files with:

```bash
pytest tests/test_ingestion.py
```

---

## Custom Configuration

- See [`config/configuration.yaml`](../config/configuration.yaml)
- Set environment variables in `.env` (based on `.env.template`)
- Configure AI backend in the `ai:` block (OpenAI, Azure, Ollama)

---

